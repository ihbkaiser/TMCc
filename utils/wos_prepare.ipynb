{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c244f914",
   "metadata": {},
   "source": [
    "Follow this notebook to setup WoS dataset   \n",
    "This notebook is prepared based on: HDLTex: Hierarchical Deep Learning for Text Classification ||| [paper](https://doi.org/10.1109/ICMLA.2017.0-134) ||| [github](https://github.com/kk7nc/HDLTex/tree/master) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a22d75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/home/ducanh/Credit/TM-clusterrin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd9c8c1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Dataset.zip to WoS_Dataset_2 ...\n"
     ]
    },
    {
     "ename": "HTTPError",
     "evalue": "403 Client Error: Forbidden for url: https://www.researchgate.net/profile/Kamran-Kowsari/publication/321038556_Web_Of_Science_Dataset/data/5a09f9daaca272d40f412017/Dataset.zip?_sg%5B0%5D=T2IX7UKFm_80V4eGOmcEHFMZtHsfBS6p-MygLIgLue98TNFPiXVMFnGx5pK4e3eAinN4Z262MwNq2w-Gtzo5tg.iy1QPikF7AeR3p2iJ887KoJAQN1DvSCD1oUiDjAsA5ib8mgfdaDPXxqeWlzJ6et-PqiMabXc5QItGMERJV4VOA&_sg%5B1%5D=avnfE9AjAykTfiJF4GtikC-t-Y7pjrrh6yHA9IyEqdSAoGnIAOpEMruo8L3cEO3110HUU6XVxPNMvIJniYf5Mp5P5Dg6gQgLTlp14INYDaki.iy1QPikF7AeR3p2iJ887KoJAQN1DvSCD1oUiDjAsA5ib8mgfdaDPXxqeWlzJ6et-PqiMabXc5QItGMERJV4VOA&_iepl=",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 58\u001b[39m\n\u001b[32m     56\u001b[39m DATA_URL = \u001b[33m'\u001b[39m\u001b[33mhttps://www.researchgate.net/profile/Kamran-Kowsari/publication/321038556_Web_Of_Science_Dataset/data/5a09f9daaca272d40f412017/Dataset.zip?_sg\u001b[39m\u001b[33m%\u001b[39m\u001b[33m5B0\u001b[39m\u001b[33m%\u001b[39m\u001b[33m5D=T2IX7UKFm_80V4eGOmcEHFMZtHsfBS6p-MygLIgLue98TNFPiXVMFnGx5pK4e3eAinN4Z262MwNq2w-Gtzo5tg.iy1QPikF7AeR3p2iJ887KoJAQN1DvSCD1oUiDjAsA5ib8mgfdaDPXxqeWlzJ6et-PqiMabXc5QItGMERJV4VOA&_sg\u001b[39m\u001b[33m%\u001b[39m\u001b[33m5B1\u001b[39m\u001b[33m%\u001b[39m\u001b[33m5D=avnfE9AjAykTfiJF4GtikC-t-Y7pjrrh6yHA9IyEqdSAoGnIAOpEMruo8L3cEO3110HUU6XVxPNMvIJniYf5Mp5P5Dg6gQgLTlp14INYDaki.iy1QPikF7AeR3p2iJ887KoJAQN1DvSCD1oUiDjAsA5ib8mgfdaDPXxqeWlzJ6et-PqiMabXc5QItGMERJV4VOA&_iepl=\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     57\u001b[39m output_dir = \u001b[33m\"\u001b[39m\u001b[33mWoS_Dataset_2\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m out_dir = \u001b[43mdownload_and_extract\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDATA_URL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mData is ready in: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mout_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 28\u001b[39m, in \u001b[36mdownload_and_extract\u001b[39m\u001b[34m(url, output_dir)\u001b[39m\n\u001b[32m     26\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDownloading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m ...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m requests.get(url, headers=headers, stream=\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m r:\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m     \u001b[43mr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m     total = \u001b[38;5;28mint\u001b[39m(r.headers.get(\u001b[33m'\u001b[39m\u001b[33mcontent-length\u001b[39m\u001b[33m'\u001b[39m, \u001b[32m0\u001b[39m))\n\u001b[32m     30\u001b[39m     downloaded = \u001b[32m0\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Credit/TM-clusterrin/.conda/lib/python3.11/site-packages/requests/models.py:1024\u001b[39m, in \u001b[36mResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1019\u001b[39m     http_error_msg = (\n\u001b[32m   1020\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.status_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m Server Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreason\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for url: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.url\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1021\u001b[39m     )\n\u001b[32m   1023\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[32m-> \u001b[39m\u001b[32m1024\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response=\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[31mHTTPError\u001b[39m: 403 Client Error: Forbidden for url: https://www.researchgate.net/profile/Kamran-Kowsari/publication/321038556_Web_Of_Science_Dataset/data/5a09f9daaca272d40f412017/Dataset.zip?_sg%5B0%5D=T2IX7UKFm_80V4eGOmcEHFMZtHsfBS6p-MygLIgLue98TNFPiXVMFnGx5pK4e3eAinN4Z262MwNq2w-Gtzo5tg.iy1QPikF7AeR3p2iJ887KoJAQN1DvSCD1oUiDjAsA5ib8mgfdaDPXxqeWlzJ6et-PqiMabXc5QItGMERJV4VOA&_sg%5B1%5D=avnfE9AjAykTfiJF4GtikC-t-Y7pjrrh6yHA9IyEqdSAoGnIAOpEMruo8L3cEO3110HUU6XVxPNMvIJniYf5Mp5P5Dg6gQgLTlp14INYDaki.iy1QPikF7AeR3p2iJ887KoJAQN1DvSCD1oUiDjAsA5ib8mgfdaDPXxqeWlzJ6et-PqiMabXc5QItGMERJV4VOA&_iepl="
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import sys\n",
    "import tarfile\n",
    "import zipfile\n",
    "from urllib.request import urlretrieve\n",
    "from urllib.parse import urlparse, unquote\n",
    "import requests\n",
    "\n",
    "\n",
    "def download_and_extract(url: str, output_dir: str) -> str:\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    parsed = urlparse(url)\n",
    "    filename = os.path.basename(parsed.path)\n",
    "    filepath = os.path.join(output_dir, filename)\n",
    "\n",
    "\n",
    "    if not os.path.exists(filepath):\n",
    "        headers = {\n",
    "            'User-Agent': (\n",
    "                'Mozilla/5.0 (Windows NT 10.0; Win64; x64) '\n",
    "                'AppleWebKit/537.36 (KHTML, like Gecko) '\n",
    "                'Chrome/114.0.0.0 Safari/537.36'\n",
    "            )\n",
    "        }\n",
    "        print(f\"Downloading {filename} to {output_dir} ...\")\n",
    "        with requests.get(url, headers=headers, stream=True) as r:\n",
    "            r.raise_for_status()\n",
    "            total = int(r.headers.get('content-length', 0))\n",
    "            downloaded = 0\n",
    "            with open(filepath, 'wb') as f:\n",
    "                for chunk in r.iter_content(chunk_size=8192):\n",
    "                    if not chunk:\n",
    "                        continue\n",
    "                    f.write(chunk)\n",
    "                    downloaded += len(chunk)\n",
    "                    pct = downloaded / total * 100 if total else 0\n",
    "                    sys.stdout.write(f'\\r  {pct:5.1f}%')\n",
    "                    sys.stdout.flush()\n",
    "        print(f\"\\nDownloaded: {filename}\")\n",
    "\n",
    "    if filename.endswith(('.tar.gz', '.tar')):\n",
    "        with tarfile.open(filepath, 'r:*') as archive:\n",
    "            archive.extractall(output_dir)\n",
    "            print(f'Extracted tar archive to {output_dir}')\n",
    "    elif filename.endswith('.zip'):\n",
    "        with zipfile.ZipFile(filepath, 'r') as archive:\n",
    "            archive.extractall(output_dir)\n",
    "            print(f'Extracted zip archive to {output_dir}')\n",
    "    else:\n",
    "        print(f'Unsupported archive format: {filename}')\n",
    "\n",
    "    return os.path.abspath(output_dir)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    DATA_URL = 'https://www.researchgate.net/profile/Kamran-Kowsari/publication/321038556_Web_Of_Science_Dataset/data/5a09f9daaca272d40f412017/Dataset.zip?_sg%5B0%5D=T2IX7UKFm_80V4eGOmcEHFMZtHsfBS6p-MygLIgLue98TNFPiXVMFnGx5pK4e3eAinN4Z262MwNq2w-Gtzo5tg.iy1QPikF7AeR3p2iJ887KoJAQN1DvSCD1oUiDjAsA5ib8mgfdaDPXxqeWlzJ6et-PqiMabXc5QItGMERJV4VOA&_sg%5B1%5D=avnfE9AjAykTfiJF4GtikC-t-Y7pjrrh6yHA9IyEqdSAoGnIAOpEMruo8L3cEO3110HUU6XVxPNMvIJniYf5Mp5P5Dg6gQgLTlp14INYDaki.iy1QPikF7AeR3p2iJ887KoJAQN1DvSCD1oUiDjAsA5ib8mgfdaDPXxqeWlzJ6et-PqiMabXc5QItGMERJV4VOA&_iepl='\n",
    "    output_dir = \"WoS_Dataset_2\"\n",
    "    out_dir = download_and_extract(DATA_URL, output_dir)\n",
    "    print(f'Data is ready in: {out_dir}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0c2756a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from topmost.preprocess import Preprocess\n",
    "import scipy\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "path_WOS = 'utils'\n",
    "\n",
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for dataset\n",
    "    Every dataset is lower cased except\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"\\\\\", \"\", string)\n",
    "    string = re.sub(r\"\\'\", \"\", string)\n",
    "    string = re.sub(r\"\\\"\", \"\", string)\n",
    "    return string.strip().lower()\n",
    "\n",
    "def text_cleaner(text):\n",
    "    \"\"\"\n",
    "    cleaning spaces, html tags, etc\n",
    "    parameters: (string) text input to clean\n",
    "    return: (string) clean_text \n",
    "    \"\"\"\n",
    "    text = text.replace(\".\", \"\")\n",
    "    text = text.replace(\"[\", \" \")\n",
    "    text = text.replace(\",\", \" \")\n",
    "    text = text.replace(\"]\", \" \")\n",
    "    text = text.replace(\"(\", \" \")\n",
    "    text = text.replace(\")\", \" \")\n",
    "    text = text.replace(\"\\\"\", \"\")\n",
    "    text = text.replace(\"-\", \"\")\n",
    "    text = text.replace(\"=\", \"\")\n",
    "    rules = [\n",
    "        {r'>\\s+': u'>'},  # remove spaces after a tag opens or closes\n",
    "        {r'\\s+': u' '},  # replace consecutive spaces\n",
    "        {r'\\s*<br\\s*/?>\\s*': u'\\n'},  # newline after a <br>\n",
    "        {r'</(div)\\s*>\\s*': u'\\n'},  # newline after </p> and </div> and <h1/>...\n",
    "        {r'</(p|h\\d)\\s*>\\s*': u'\\n\\n'},  # newline after </p> and </div> and <h1/>...\n",
    "        {r'<head>.*<\\s*(/head|body)[^>]*>': u''},  # remove <head> to </head>\n",
    "        {r'<a\\s+href=\"([^\"]+)\"[^>]*>.*</a>': r'\\1'},  # show links instead of texts\n",
    "        {r'[ \\t]*<[^<]*?/?>': u''},  # remove remaining tags\n",
    "        {r'^\\s+': u''}  # remove spaces at the beginning\n",
    "    ]\n",
    "    for rule in rules:\n",
    "        for (k, v) in rule.items():\n",
    "            regex = re.compile(k)\n",
    "            text = regex.sub(v, text)\n",
    "        text = text.rstrip()\n",
    "        text = text.strip()\n",
    "    clean_text = text.lower()\n",
    "    return clean_text\n",
    "\n",
    "\n",
    "def loadData(path_WOS: str,\n",
    "             out_root: str = \"tm_datasets\",\n",
    "             version: str = \"small\"):\n",
    "    assert version in [\"small\", \"medium\", \"large\"], f\"Version {version} is not supported. Choose from ['small', 'medium', 'large']\"\n",
    "    version_num = \"5736\" if version == \"small\" else \"11967\" if version == \"medium\" else \"46985\"\n",
    "\n",
    "    \"\"\"\n",
    "    path_WOS : str\n",
    "        path to the WoS dataset downloaded before, e.g. \"download/WoS_Dataset\"\n",
    "    out_root : str\n",
    "        path to the output directory where the processed data will be saved, this will\n",
    "        create a subdirectory WOS_{version} in out_root\n",
    "    version : str\n",
    "        version of the dataset, \"small\" for 5736 documents, \"medium\" for 11967 documents,\n",
    "        \"large\" for 46985 documents\n",
    "    \"\"\"\n",
    "    out_dir = os.path.join(out_root, f\"WOS_{version}\")\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    fname = os.path.join(path_WOS,f\"WOS{version_num}/X.txt\")\n",
    "    fnamek = os.path.join(path_WOS,f\"WOS{version_num}/Y.txt\")\n",
    "    with open(fname) as f:\n",
    "        content = f.readlines()\n",
    "        content = [text_cleaner(x) for x in content]\n",
    "    with open(fnamek) as fk:\n",
    "        contentk = fk.readlines()\n",
    "    contentk = [x.strip() for x in contentk]\n",
    "    Label = np.matrix(contentk, dtype=int)\n",
    "    Label = np.transpose(Label)\n",
    "    number_of_classes_L1 = np.max(Label)+1  # number of classes in Level 1\n",
    "    print('Number of classes in Level 1:', number_of_classes_L1)\n",
    "\n",
    "    np.random.seed(7)  # lucky number =))\n",
    "    print(Label.shape)\n",
    "\n",
    "    # bow\n",
    "    X_train_te, X_test_te, y_train, y_test  = train_test_split(content, Label, test_size=0.2,random_state= 0)\n",
    "    # preprocess phase 2 via topmost\n",
    "    preprocessor = Preprocess(vocab_size=5000)\n",
    "    rst_phase2 = preprocessor.preprocess(raw_train_texts=X_train_te,\n",
    "                                         raw_test_texts=X_test_te,\n",
    "                                         pretrained_WE=True)\n",
    "    \n",
    "    X_train_te = rst_phase2['train_texts']\n",
    "    X_test_te = rst_phase2['test_texts']\n",
    "    X_train = rst_phase2['train_bow']\n",
    "    X_test = rst_phase2['test_bow']\n",
    "    with open(os.path.join(out_dir, \"train_raw.txt\"), \"w\", encoding=\"utf8\") as f:\n",
    "        f.write(\"\\n\".join(X_train_te))\n",
    "    with open(os.path.join(out_dir, \"test_raw.txt\"), \"w\", encoding=\"utf8\") as f:\n",
    "        f.write(\"\\n\".join(X_test_te))\n",
    "    vocab = rst_phase2['vocab']\n",
    "    word_embeddings = rst_phase2['word_embeddings']\n",
    "    print('Vocabulary size:', len(vocab))\n",
    "    # save vocab\n",
    "    with open(os.path.join(out_dir, \"vocab.txt\"), \"w\", encoding=\"utf8\") as f:\n",
    "        for token in vocab:\n",
    "            f.write(token + \"\\n\")\n",
    "    # save texts\n",
    "    with open(os.path.join(out_dir, \"train_texts.txt\"), \"w\", encoding=\"utf8\") as f:\n",
    "        f.write(\"\\n\".join(X_train_te))\n",
    "    with open(os.path.join(out_dir, \"test_texts.txt\"), \"w\", encoding=\"utf8\") as f:\n",
    "        f.write(\"\\n\".join(X_test_te))\n",
    "    # save labels\n",
    "    with open(os.path.join(out_dir, \"train_labels.txt\"), \"w\", encoding=\"utf8\") as f:\n",
    "        for label in y_train:\n",
    "            f.write(str(int(label)) + \"\\n\")\n",
    "    with open(os.path.join(out_dir, \"test_labels.txt\"), \"w\", encoding=\"utf8\") as f:\n",
    "        for label in y_test:\n",
    "            f.write(str(int(label)) + \"\\n\")\n",
    "    # save bag of words\n",
    "    scipy.sparse.save_npz(f\"{out_dir}/train_bow.npz\", scipy.sparse.csr_matrix(X_train))\n",
    "    scipy.sparse.save_npz(f\"{out_dir}/test_bow.npz\", scipy.sparse.csr_matrix(X_test))\n",
    "    # word embeddings\n",
    "    scipy.sparse.save_npz(f\"{out_dir}/word_embeddings.npz\", word_embeddings)\n",
    "    \n",
    "\n",
    "    return (X_train,y_train,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0061c5e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes in Level 1: 33\n",
      "(11967, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading train texts: 100%|██████████| 9573/9573 [00:01<00:00, 5433.04it/s]\n",
      "loading test texts: 100%|██████████| 2394/2394 [00:00<00:00, 5194.83it/s]\n",
      "parsing texts: 100%|██████████| 9573/9573 [00:01<00:00, 8108.70it/s]\n",
      "2025-08-19 00:40:46,827 - TopMost - Real vocab size: 5000\n",
      "2025-08-19 00:40:46,828 - TopMost - Real training size: 9573 \t avg length: 88.166\n",
      "parsing texts: 100%|██████████| 2394/2394 [00:00<00:00, 8239.74it/s]\n",
      "2025-08-19 00:40:47,214 - TopMost - Real testing size: 2394 \t avg length: 87.471\n",
      "loading word embeddings: 100%|██████████| 5000/5000 [00:00<00:00, 6409.60it/s]\n",
      "2025-08-19 00:41:21,422 - TopMost - number of found embeddings: 4927/5000\n",
      "/tmp/ipykernel_2972118/1766999502.py:122: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  f.write(str(int(label)) + \"\\n\")\n",
      "/tmp/ipykernel_2972118/1766999502.py:125: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  f.write(str(int(label)) + \"\\n\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 5000\n"
     ]
    }
   ],
   "source": [
    "dummy1 = loadData(path_WOS = \"WOS_new\",\n",
    "                 out_root = \"tm_datasets\",\n",
    "                 version = \"medium\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
